{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\projects\\Sentiment project\\backend\\train.tsv\\train.tsv\",  sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase   \n",
       "0         1           1  A series of escapades demonstrating the adage ...  \\\n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "5         6           1  of escapades demonstrating the adage that what...   \n",
       "6         7           1                                                 of   \n",
       "7         8           1  escapades demonstrating the adage that what is...   \n",
       "8         9           1                                          escapades   \n",
       "9        10           1  demonstrating the adage that what is good for ...   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  \n",
       "5          2  \n",
       "6          2  \n",
       "7          2  \n",
       "8          2  \n",
       "9          2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6098936306548763\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.35      1416\n",
      "           1       0.51      0.40      0.45      5527\n",
      "           2       0.68      0.80      0.73     15639\n",
      "           3       0.52      0.49      0.51      6707\n",
      "           4       0.49      0.29      0.36      1923\n",
      "\n",
      "    accuracy                           0.61     31212\n",
      "   macro avg       0.53      0.45      0.48     31212\n",
      "weighted avg       0.59      0.61      0.59     31212\n",
      "\n",
      "Sentence: 'This movie was fantastic!', Predicted Sentiment: 3\n",
      "Sentence: 'The acting was terrible.', Predicted Sentiment: 0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "# Download NLTK data (if needed)\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Processed_Phrase'] = df['Phrase'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['Processed_Phrase']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text Vectorization (using a simple bag-of-words approach)\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Model Building: Naive Bayes classifier\n",
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = nb_classifier.predict(X_test_vectorized)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict sentiment for new input\n",
    "def predict_sentiment(input_text):\n",
    "    input_text = preprocess_text(input_text)\n",
    "    input_vectorized = vectorizer.transform([input_text])\n",
    "    predicted_sentiment = nb_classifier.predict(input_vectorized)[0]\n",
    "    return predicted_sentiment\n",
    "\n",
    "# Predict sentiment for new input phrases\n",
    "new_inputs = [\"This movie was fantastic!\", \"The acting was terrible.\"]\n",
    "for input_text in new_inputs:\n",
    "    predicted_sentiment = predict_sentiment(input_text)\n",
    "    print(f\"Sentence: '{input_text}', Predicted Sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6271946687171601\n",
      "Classification Report:\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "         negative       0.60      0.18      0.28      1416\n",
      "          neutral       0.66      0.88      0.76     15639\n",
      "         positive       0.62      0.24      0.34      1923\n",
      "somewhat negative       0.54      0.35      0.42      5527\n",
      "somewhat positive       0.57      0.47      0.51      6707\n",
      "\n",
      "         accuracy                           0.63     31212\n",
      "        macro avg       0.60      0.42      0.46     31212\n",
      "     weighted avg       0.61      0.63      0.60     31212\n",
      "\n",
      "Sentence: 'This movie was fantastic!', Predicted Sentiment: positive\n",
      "Sentence: 'The acting was terrible.', Predicted Sentiment: negative\n",
      "Sentence: 'The weather ruined our plans for the day', Predicted Sentiment: neutral\n",
      "Sentence: 'The presentation could have been better, but it wasn't the worst.', Predicted Sentiment: somewhat negative\n",
      "Sentence: 'The conference started at 10 AM.', Predicted Sentiment: neutral\n",
      "Sentence: 'The event was enjoyable; I had a good time.', Predicted Sentiment: somewhat positive\n",
      "Sentence: 'I loved the movie; it was absolutely fantastic!', Predicted Sentiment: positive\n",
      "Sentence: 'The play was outstanding, and I would definitely watch it again.', Predicted Sentiment: positive\n",
      "Sentence: 'I went for a walk in the park today.', Predicted Sentiment: neutral\n",
      "Sentence: 'The movie was of average length.', Predicted Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download NLTK data (if needed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "# Map sentiment labels to 0-4 scale\n",
    "sentiment_mapping = {\n",
    "    0: 'negative',\n",
    "    1: 'somewhat negative',\n",
    "    2: 'neutral',\n",
    "    3: 'somewhat positive',\n",
    "    4: 'positive'\n",
    "}\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Processed_Phrase'] = df['Phrase'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['Processed_Phrase']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text Vectorization using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Model Building: Logistic Regression as an example\n",
    "model = LogisticRegression(max_iter=5000)  # Increase max_iter for convergence\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Predict sentiment for new input\n",
    "def predict_sentiment(input_text):\n",
    "    input_text = preprocess_text(input_text)\n",
    "    input_vectorized = vectorizer.transform([input_text])\n",
    "    predicted_sentiment = model.predict(input_vectorized)[0]\n",
    "    return predicted_sentiment\n",
    "\n",
    "# Predict sentiment for new input phrases\n",
    "new_inputs = [\"This movie was fantastic!\", \"The acting was terrible.\",\"The weather ruined our plans for the day\",\"The presentation could have been better, but it wasn't the worst.\",\"The conference started at 10 AM.\",\"The event was enjoyable; I had a good time.\",\"I loved the movie; it was absolutely fantastic!\",\"The play was outstanding, and I would definitely watch it again.\",\"I went for a walk in the park today.\",\"The movie was of average length.\"\n",
    "]\n",
    "for input_text in new_inputs:\n",
    "    predicted_sentiment = predict_sentiment(input_text)\n",
    "    print(f\"Sentence: '{input_text}', Predicted Sentiment: {predicted_sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'This movie was fantastic!', Predicted Sentiment: positive\n",
      "Sentence: 'The acting was terrible.', Predicted Sentiment: negative\n",
      "Sentence: 'The weather ruined our plans for the day', Predicted Sentiment: neutral\n",
      "Sentence: 'The presentation could have been better, but it wasn't the worst.', Predicted Sentiment: somewhat negative\n",
      "Sentence: 'The conference started at 10 AM.', Predicted Sentiment: neutral\n",
      "Sentence: 'The event was enjoyable; I had a good time.', Predicted Sentiment: somewhat positive\n",
      "Sentence: 'I loved the movie; it was absolutely fantastic!', Predicted Sentiment: positive\n",
      "Sentence: 'The play was outstanding, and I would definitely watch it again.', Predicted Sentiment: positive\n",
      "Sentence: 'I went for a walk in the park today.', Predicted Sentiment: neutral\n",
      "Sentence: 'The movie was of average length.', Predicted Sentiment: neutral\n",
      "Sentence: 'great', Predicted Sentiment: somewhat positive\n",
      "Sentence: 'not great', Predicted Sentiment: somewhat positive\n",
      "Sentence: 'this is awesome', Predicted Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for new input phrases\n",
    "new_inputs = [\"This movie was fantastic!\", \"The acting was terrible.\",\"The weather ruined our plans for the day\",\"The presentation could have been better, but it wasn't the worst.\",\"The conference started at 10 AM.\",\"The event was enjoyable; I had a good time.\",\"I loved the movie; it was absolutely fantastic!\",\"The play was outstanding, and I would definitely watch it again.\",\"I went for a walk in the park today.\",\"The movie was of average length.\",\"great\",\"not great\",\"this is awesome\"\n",
    "]\n",
    "for input_text in new_inputs:\n",
    "    predicted_sentiment = predict_sentiment(input_text)\n",
    "    print(f\"Sentence: '{input_text}', Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'The movie was a complete disaster.', Predicted Sentiment: negative\n",
      "Sentence: 'I hated the food at that restaurant.', Predicted Sentiment: neutral\n",
      "Sentence: 'The customer service was awful, very disappointing experience.', Predicted Sentiment: negative\n",
      "Sentence: 'The traffic was unbearable, making me late for my appointment.', Predicted Sentiment: neutral\n",
      "Sentence: 'fuck you', Predicted Sentiment: neutral\n"
     ]
    }
   ],
   "source": [
    "# Predict sentiment for new input phrases\n",
    "new_inputs = [\"The movie was a complete disaster.\",\"I hated the food at that restaurant.\",\"The customer service was awful, very disappointing experience.\",\"The traffic was unbearable, making me late for my appointment.\",\"fuck you\"]\n",
    "for input_text in new_inputs:\n",
    "    predicted_sentiment = predict_sentiment(input_text)\n",
    "    print(f\"Sentence: '{input_text}', Predicted Sentiment: {predicted_sentiment}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\projects\\Sentiment project\\backend\\train.tsv\\train.tsv\",  sep='\\t')\n",
    "\n",
    "# Map sentiment labels to 0-4 scale\n",
    "sentiment_mapping = {\n",
    "    0: 'negative',\n",
    "    1: 'somewhat negative',\n",
    "    2: 'neutral',\n",
    "    3: 'somewhat positive',\n",
    "    4: 'positive'\n",
    "}\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Processed_Phrase'] = df['Phrase'].apply(preprocess_text)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X = df['Processed_Phrase']\n",
    "y = df['Sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Text Vectorization using TF-IDF with n-grams (1, 2)\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Model Building: Random Forest Classifier with class weights\n",
    "model = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 28, 128)           1861120   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               91600     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 505       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1953225 (7.45 MB)\n",
      "Trainable params: 1953225 (7.45 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "1707/1707 [==============================] - 133s 76ms/step - loss: 0.9858 - accuracy: 0.6059 - val_loss: 0.8821 - val_accuracy: 0.6449\n",
      "Epoch 2/5\n",
      "1707/1707 [==============================] - 118s 69ms/step - loss: 0.8059 - accuracy: 0.6731 - val_loss: 0.8584 - val_accuracy: 0.6562\n",
      "Epoch 3/5\n",
      "1707/1707 [==============================] - 122s 72ms/step - loss: 0.7247 - accuracy: 0.6998 - val_loss: 0.8613 - val_accuracy: 0.6574\n",
      "Epoch 4/5\n",
      "1707/1707 [==============================] - 120s 70ms/step - loss: 0.6646 - accuracy: 0.7218 - val_loss: 0.8939 - val_accuracy: 0.6546\n",
      "Epoch 5/5\n",
      "1707/1707 [==============================] - 119s 70ms/step - loss: 0.6186 - accuracy: 0.7363 - val_loss: 0.9374 - val_accuracy: 0.6470\n",
      "1464/1464 [==============================] - 14s 9ms/step\n",
      "Accuracy: 0.6469733863044128\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.45      2113\n",
      "           1       0.72      0.80      0.76     23588\n",
      "           2       0.50      0.50      0.50      2848\n",
      "           3       0.54      0.47      0.51      8228\n",
      "           4       0.58      0.51      0.54     10041\n",
      "\n",
      "    accuracy                           0.65     46818\n",
      "   macro avg       0.57      0.54      0.55     46818\n",
      "weighted avg       0.64      0.65      0.64     46818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(r\"C:\\Users\\User\\Desktop\\projects\\Sentiment project\\backend\\train.tsv\\train.tsv\",  sep='\\t')\n",
    "\n",
    "# Map sentiment labels to 0-4 scale\n",
    "sentiment_mapping = {\n",
    "    0: 'negative',\n",
    "    1: 'somewhat negative',\n",
    "    2: 'neutral',\n",
    "    3: 'somewhat positive',\n",
    "    4: 'positive'\n",
    "}\n",
    "\n",
    "df['Sentiment'] = df['Sentiment'].map(sentiment_mapping)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower())  # Tokenize and lowercase\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Remove non-alphabetic tokens\n",
    "    tokens = [t for t in tokens if t not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "df['Processed_Phrase'] = df['Phrase'].apply(preprocess_text)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['Sentiment'])\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Processed_Phrase'])\n",
    "X = tokenizer.texts_to_sequences(df['Processed_Phrase'])\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Model Building: LSTM\n",
    "embedding_dim = 128\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "max_length = X.shape[1]\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model.add(LSTM(units=100))\n",
    "model.add(Dense(5, activation='softmax'))  # 5 classes for sentiment\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)\n",
    "\n",
    "# Model Evaluation\n",
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976/976 [==============================] - 8s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred_prob = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_prob, axis=1)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6469733863044128\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.42      0.45      2113\n",
      "           1       0.72      0.80      0.76     23588\n",
      "           2       0.50      0.50      0.50      2848\n",
      "           3       0.54      0.47      0.51      8228\n",
      "           4       0.58      0.51      0.54     10041\n",
      "\n",
      "    accuracy                           0.65     46818\n",
      "   macro avg       0.57      0.54      0.55     46818\n",
      "weighted avg       0.64      0.65      0.64     46818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the model to disk\n",
    "with open('lstm_model.pkl', 'wb') as model_file:\n",
    "    pickle.dump(model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tokenizer to a file\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
